**Некоторые приемы для Wget**

---

Копирование сайта для локального просмотра:

wget -b -c -w5 -r -l0 -np --relative -k [http://www.poplinux.ru/](http://www.poplinux.ru/)

где:**-b** копирование будет выполняться в фоне, с выводов итогов выполнения в файл **wget-log-c** возобновление закачки после обрыва связи с сайтому, чтобы она сама самовозобновлялась.**-w5** копирование с паузами в 5 сек (меньшая нагрузка на сайт)**-l5** глубина скачивание сайта (0 - на всю глубину сайта)**-np** (**--no-parent**) запрещать скачивание выше заданного адреса**--relative** ходить только по относительным ссылкам (не скачивать с других сайтов)**-k** конвертация абсолютных ссылок в локальные

Копирование Web-сайта со структурой (grabbing):

1.  # Непрерывное копирование (высокая скорость):
    
2.  wget --no-parent -r -l 0 -k [http://www.geocities.com/airfly/](http://www.geocities.com/airfly/)
    
3.  # Копирование с паузами 1 сек (меньше нагрузка на сайт)
    
4.  wget -w 1 --no-parent -r -l 0 -k [http://www.geocities.com/airfly/](http://www.geocities.com/airfly/)
    

Если добавить ключ **-b** копирование будет выполняться в фоне, а итоги выполнения выводиться в файл **wget-log**

Если связь с сайтом нарушится, для возобновления копирования с прерванного места нужно добавить ключик **-c**либо сразу внести его в команду закачку, чтобы она сама самовозобновлялась.

Чтобы выкачать файлы из списка, содержащего прямые ссылки:

1.  wget -i linklist
2.  # или
    
3.  wget -input-file=linklist

Здесь указывается только файл, в котором содержатся ссылки.Файл может также быть HTML-страницей, в которой есть ссылки. Они будут выкачаны указанной выше командой.

Зеркалирование сайтов на локальную машину:

wget -m [http://www.poplinux.ru/](http://www.poplinux.ru/)

При этом ссылки останутся абсолютными, то есть будут указывать на Интернет-адреса, поэтому просматривать их на локальной машине будет затруднительно.

Копирование сайта для локального просмотра:

wget -r -l0 -np -k [http://www.vasyapupkin.com/](http://www.vasyapupkin.com/)

При этом будет включена рекурсивная выгрузка (ключ **-r**, **--recursive**), то есть не только файлы с главной страницы, но и все остальные, на которые ведут ссылки (ключ **-l0** бесконечная вложенность ссылок).

Имена ссылок будут переконвертированы в локальные для удобства просмотра (ключ **-k**). Также при помощи ключа**-np** (**no-parrent**) можно запретить wget подниматься выше начального адреса при рекурсивной загрузке. То есть, если вы копируете [](http://poplinux.ru/)[http://poplinux.ru/](http://poplinux.ru/) то по ссылкам с основного сайта [](http://www.www.poplinux.ru//)[http://www.www.poplinux.ru//](http://www.www.poplinux.ru//) скопированы не будут.

Включение и исключение файлов при загрузке.

Задает разделяемые запятыми шаблоны имен файлов, которые следует загружать (**acclist**) или игнорировать (**rejlist**):

-   A acclist | -accept acclist
-   R rejlist | -reject rejlist

Превращает абсолютные ссылки (типа [](http://www/)[http://www](http://www)...) в относительные (типа file///home/abdula/www/index.html) для удобства локального просмотра. Чтобы локально в броузере просмотреть скачанный сайт, открываете файл index.html в броузере и бродите по ссылкам точно так же, как если бы вы были подключены к Интернету:

-   k | -convert-links

Разрешает wget скачивать данные с любого адреса, на который есть ссылка в запрашиваемом документе:

-   H --span-hosts

Загружать все файлы, которые нужны для отображения страниц HTML. Например: рисунки, звук и каскадные стили. После завершения загрузки конвертировать ссылки в документе для просмотра в автономном режиме. Это касается не только видимых ссылок на другие документы, а ссылок на все внешние локальные файлы:

-   p --page-requisites